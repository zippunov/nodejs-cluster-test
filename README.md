## Architecture

There where two most critical assignment requirements dictate the architectural solution.

- Large payload size
- Requirement to respond to the REST API call without waiting for data to be saved.

This leads to the following consideration:

- Under high load, having a comparatively large request payload might block other REST calls while the NodeJS thread piping a large stream. It will be blocked not in a sense of Blocking IO but the incoming large stream always will be ready with the next buffer processing callbacks.

  So I decided to implement NodeJS Cluster with a set of identical NodeJS workers.

- As we respond immediately to client requests without saving data, we need to store collected data for further processing in the queue.

- Considering that this is just a coding exercise, not a real-world application, I am using the in-memory queue.

  In the real application, I would choose a real persistent queue like Kafka or NATS. e.t.c

- With this architecture, it becomes extremely important to have a proper graceful service shutdown.

![Architecture diagram](/doc/diagram.png)

## Web UI

Available at relative URL `/ui`

Writing frontends with bare JS and HTML was never my strong point.

I am quite comfortable with React or Svelte. But using those will significantly complicate project structure. So I decided against it.

I did try my best to make a basic frontend using an EJS template engine. The only purpose of using templates is to transfer application settings (namely service PORT number).

## Configuration

The application expects a number of environment variables to be set in order to run:

```text
PORT    - Service port (default 9932)
PG_USER - Postgres username (default 'postgres')
PG_PASS - Postgres password (default 'postgres')
PG_HOST - Postgres host (default 'localhost')
PG_PORT - Postgres port (default 5432)
PG_DB   - Postrgres database (default 'test')
```

## Database

During start-up, the service runs data migration creating (if not exist) a new table:

```SQL
CREATE TABLE IF NOT EXISTS post_data (
  "id" BIGINT NOT NULL GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
  "timestamp" TIMESTAMP NOT NULL,
  "data" TEXT
);
```

Again, there are a plethora of rich migration engines in the NodeJs universe. In the real-world scenario, I will use one of them. I have decided to go with the simplest working solution in order to avoid unnecessary project complications.

## How to run

- Ensure that nodeJs version is >= 16
- Install dependencies
```bash
npm install
```
- run service. (Web frontend will be automatically opened in 5 sec.)

```bash
PORT=8989 \
PG_USER=gosurf \
PG_PASS=gosurf \
PG_HOST=localhost \
PG_PORT=5432 \
PG_DB=surf \
npm run dev;
```

## Load Tests

I have created simple load test with `autocannon` framework. It sends 5K requests in ~10 seconds to the running service.
I have chosen random payload size to be 100Kb.

### Test execution

Run service:
```bash
PORT=8989 \
PG_USER=gosurf \
PG_PASS=gosurf \
PG_HOST=localhost \
PG_PORT=5432 \
PG_DB=surf \
npm run dev;
```
Run load test in the separate console window:
```bash
PORT=8989 npm run load-test
```
In 10 seconds resulting report is getting generated:
```bash

┌─────────┬──────┬───────┬───────┬───────┬──────────┬─────────┬────────┐
│ Stat    │ 2.5% │ 50%   │ 97.5% │ 99%   │ Avg      │ Stdev   │ Max    │
├─────────┼──────┼───────┼───────┼───────┼──────────┼─────────┼────────┤
│ Latency │ 2 ms │ 18 ms │ 39 ms │ 43 ms │ 18.82 ms │ 8.69 ms │ 103 ms │
└─────────┴──────┴───────┴───────┴───────┴──────────┴─────────┴────────┘
┌───────────┬─────────┬─────────┬─────────┬─────────┬─────────┬───────┬─────────┐
│ Stat      │ 1%      │ 2.5%    │ 50%     │ 97.5%   │ Avg     │ Stdev │ Min     │
├───────────┼─────────┼─────────┼─────────┼─────────┼─────────┼───────┼─────────┤
│ Req/Sec   │ 161     │ 161     │ 434     │ 459     │ 416.67  │ 78.38 │ 161     │
├───────────┼─────────┼─────────┼─────────┼─────────┼─────────┼───────┼─────────┤
│ Bytes/Sec │ 26.7 kB │ 26.7 kB │ 72.1 kB │ 76.2 kB │ 69.2 kB │ 13 kB │ 26.7 kB │
└───────────┴─────────┴─────────┴─────────┴─────────┴─────────┴───────┴─────────┘
┌──────┬───────┐
│ Code │ Count │
├──────┼───────┤
│ 200  │ 5000  │
└──────┴───────┘

Req/Bytes counts sampled once per second.
# of samples: 12

┌────────────┬──────────────┐
│ Percentile │ Latency (ms) │
├────────────┼──────────────┤
│ 0.001      │ 0            │
├────────────┼──────────────┤
│ 0.01       │ 0            │
├────────────┼──────────────┤
│ 0.1        │ 1            │
├────────────┼──────────────┤
│ 1          │ 1            │
├────────────┼──────────────┤
│ 2.5        │ 2            │
├────────────┼──────────────┤
│ 10         │ 15           │
├────────────┼──────────────┤
│ 25         │ 16           │
├────────────┼──────────────┤
│ 50         │ 18           │
├────────────┼──────────────┤
│ 75         │ 19           │
├────────────┼──────────────┤
│ 90         │ 31           │
├────────────┼──────────────┤
│ 97.5       │ 39           │
├────────────┼──────────────┤
│ 99         │ 43           │
├────────────┼──────────────┤
│ 99.9       │ 61           │
├────────────┼──────────────┤
│ 99.99      │ 103          │
├────────────┼──────────────┤
│ 99.999     │ 103          │
└────────────┴──────────────┘

5k requests in 12.22s, 830 kB read
```
So we can see that with 100Kb payload average latency is about 20 milliseconds.
Totally we have sent 5000 requests in 12.22 seconds. All completed with status 200.
Postgres DB shows that all 5000 data records was successfully saved in the table. 
